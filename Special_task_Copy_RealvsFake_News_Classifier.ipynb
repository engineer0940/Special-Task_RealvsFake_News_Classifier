{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13702d4f-f6b9-4d34-8497-b6fbd0025e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a GPU with the name: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(\"Found a GPU with the name:\", gpu)\n",
    "else:\n",
    "    print(\"Failed to detect a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "708e6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense, Embedding, Input\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import nltk\n",
    "#nltk.download('popular')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "606ef0c8-0fbd-48ac-8319-d27d88367991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>site_url</th>\n",
       "      <th>main_img_url</th>\n",
       "      <th>type</th>\n",
       "      <th>label</th>\n",
       "      <th>title_without_stopwords</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>hasImage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barracuda Brigade</td>\n",
       "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
       "      <td>muslims busted they stole millions in govt ben...</td>\n",
       "      <td>print they should pay all the back all the mon...</td>\n",
       "      <td>english</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Real</td>\n",
       "      <td>muslims busted stole millions govt benefits</td>\n",
       "      <td>print pay back money plus interest entire fami...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reasoning with facts</td>\n",
       "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
       "      <td>re why did attorney general loretta lynch plea...</td>\n",
       "      <td>why did attorney general loretta lynch plead t...</td>\n",
       "      <td>english</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Real</td>\n",
       "      <td>attorney general loretta lynch plead fifth</td>\n",
       "      <td>attorney general loretta lynch plead fifth bar...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barracuda Brigade</td>\n",
       "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
       "      <td>breaking weiner cooperating with fbi on hillar...</td>\n",
       "      <td>red state  \\nfox news sunday reported this mor...</td>\n",
       "      <td>english</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Real</td>\n",
       "      <td>breaking weiner cooperating fbi hillary email ...</td>\n",
       "      <td>red state fox news sunday reported morning ant...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fed Up</td>\n",
       "      <td>2016-11-01T05:22:00.000+02:00</td>\n",
       "      <td>pin drop speech by father of daughter kidnappe...</td>\n",
       "      <td>email kayla mueller was a prisoner and torture...</td>\n",
       "      <td>english</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Real</td>\n",
       "      <td>pin drop speech father daughter kidnapped kill...</td>\n",
       "      <td>email kayla mueller prisoner tortured isis cha...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed Up</td>\n",
       "      <td>2016-11-01T21:56:00.000+02:00</td>\n",
       "      <td>fantastic trumps  point plan to reform healthc...</td>\n",
       "      <td>email healthcare reform to make america great ...</td>\n",
       "      <td>english</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
       "      <td>bias</td>\n",
       "      <td>Real</td>\n",
       "      <td>fantastic trumps point plan reform healthcare ...</td>\n",
       "      <td>email healthcare reform make america great sin...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                      published  \\\n",
       "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n",
       "1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n",
       "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n",
       "3                Fed Up  2016-11-01T05:22:00.000+02:00   \n",
       "4                Fed Up  2016-11-01T21:56:00.000+02:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  muslims busted they stole millions in govt ben...   \n",
       "1  re why did attorney general loretta lynch plea...   \n",
       "2  breaking weiner cooperating with fbi on hillar...   \n",
       "3  pin drop speech by father of daughter kidnappe...   \n",
       "4  fantastic trumps  point plan to reform healthc...   \n",
       "\n",
       "                                                text language  \\\n",
       "0  print they should pay all the back all the mon...  english   \n",
       "1  why did attorney general loretta lynch plead t...  english   \n",
       "2  red state  \\nfox news sunday reported this mor...  english   \n",
       "3  email kayla mueller was a prisoner and torture...  english   \n",
       "4  email healthcare reform to make america great ...  english   \n",
       "\n",
       "              site_url                                       main_img_url  \\\n",
       "0  100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/10/Fu...   \n",
       "1  100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/10/Fu...   \n",
       "2  100percentfedup.com  http://bb4sp.com/wp-content/uploads/2016/10/Fu...   \n",
       "3  100percentfedup.com  http://100percentfedup.com/wp-content/uploads/...   \n",
       "4  100percentfedup.com  http://100percentfedup.com/wp-content/uploads/...   \n",
       "\n",
       "   type label                            title_without_stopwords  \\\n",
       "0  bias  Real        muslims busted stole millions govt benefits   \n",
       "1  bias  Real         attorney general loretta lynch plead fifth   \n",
       "2  bias  Real  breaking weiner cooperating fbi hillary email ...   \n",
       "3  bias  Real  pin drop speech father daughter kidnapped kill...   \n",
       "4  bias  Real  fantastic trumps point plan reform healthcare ...   \n",
       "\n",
       "                              text_without_stopwords  hasImage  \n",
       "0  print pay back money plus interest entire fami...       1.0  \n",
       "1  attorney general loretta lynch plead fifth bar...       1.0  \n",
       "2  red state fox news sunday reported morning ant...       1.0  \n",
       "3  email kayla mueller prisoner tortured isis cha...       1.0  \n",
       "4  email healthcare reform make america great sin...       1.0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'D:\\Deep Learning Internship_Codexcue\\Special Task 1_RealvsFakeNews\\news_articles.csv\\news_articles.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61fbda05-526e-493d-80e3-13c33120331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "#df = df.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "123a4dac-d50a-48e9-bf9a-4fdec3fa2dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       english\n",
       "1       english\n",
       "2       english\n",
       "3       english\n",
       "4       english\n",
       "         ...   \n",
       "2041    english\n",
       "2042    english\n",
       "2043    english\n",
       "2044    english\n",
       "2045    english\n",
       "Name: language, Length: 2045, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "16eadf87-13ab-4f3b-aff4-237f5a9fcac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['title','text','site_url','main_img_url'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3269e4cc-c5a2-47ad-a247-a339845bbe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "english    1967\n",
      "german       72\n",
      "ignore        3\n",
      "french        2\n",
      "spanish       1\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "Fake    1291\n",
      "Real     754\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "bs            598\n",
      "conspiracy    430\n",
      "bias          389\n",
      "hate          244\n",
      "satire        146\n",
      "state         121\n",
      "junksci       102\n",
      "fake           15\n",
      "Name: count, dtype: int64\n",
      "hasImage\n",
      "1.0    1580\n",
      "0.0     465\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['language'].value_counts())\n",
    "print(df['label'].value_counts())\n",
    "print(df['type'].value_counts())\n",
    "print(df['hasImage'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "408385c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed data\n",
      "                 author                      published language  type label  \\\n",
      "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00  english  bias  Real   \n",
      "1  reasoning with facts  2016-10-29T08:47:11.259+03:00  english  bias  Real   \n",
      "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00  english  bias  Real   \n",
      "\n",
      "                             title_without_stopwords  \\\n",
      "0           muslim busted stole million govt benefit   \n",
      "1         attorney general loretta lynch plead fifth   \n",
      "2  breaking weiner cooperating fbi hillary email ...   \n",
      "\n",
      "                              text_without_stopwords  hasImage  \n",
      "0  print pay back money plus interest entire fami...       1.0  \n",
      "1  attorney general loretta lynch plead fifth bar...       1.0  \n",
      "2  red state fox news sunday reported morning ant...       1.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing function\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "# Apply preprocessing to the 'review' column\n",
    "df['title_without_stopwords'] = df['title_without_stopwords'].apply(preprocess_text)\n",
    "df['text_without_stopwords'] = df['text_without_stopwords'].apply(preprocess_text)\n",
    "print ('preprocessed data')\n",
    "# Display the preprocessed DataFrame\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "369aa643-7f78-4105-8c49-022fca94508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    print pay back money plus interest entire fami...\n",
      "1    attorney general loretta lynch plead fifth bar...\n",
      "2    red state fox news sunday reported morning ant...\n",
      "3    email kayla mueller prisoner tortured isi chan...\n",
      "4    email healthcare reform make america great sin...\n",
      "Name: text_without_stopwords, dtype: object\n",
      "label\n",
      "0.0    1291\n",
      "1.0     754\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HF\\AppData\\Local\\Temp\\ipykernel_8472\\186706190.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace({'label': {'Fake':0.0}}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.replace({'label': {'Real':1.0}}, inplace=True)\n",
    "df.replace({'label': {'Fake':0.0}}, inplace=True)\n",
    "#df['label'].replace('Real', 1.0, inplace=True)\n",
    "#df['label'].replace('Fake', 0.0, inplace=True)\n",
    "Y=df['label']\n",
    "df=df.drop('label',axis=1)\n",
    "X=df['text_without_stopwords']\n",
    "print(X.head())\n",
    "print(Y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1b2c35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42238\n"
     ]
    }
   ],
   "source": [
    "#vocab_size=5000\n",
    "tokenizer = Tokenizer( )\n",
    "#tokenizer = Tokenizer(num_words=vocab_size )\n",
    "tokenizer.fit_on_texts(X)\n",
    "w_idx=tokenizer.word_index\n",
    "vocab_size=len(w_idx)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6220aed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2045"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count\n",
    "sequences = tokenizer.texts_to_sequences(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f4f2739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2045"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "80b951dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of a review::  3015\n",
      "Min length of a review::  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2045, 150)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "# Get the minimum and the maximum length of reviews\n",
    "print(\"Max length of a review:: \", len(max((sequences), key=len)))\n",
    "print(\"Min length of a review:: \", len(min((sequences), key=len)))\n",
    "# Keeping a fixed length of all reviews to max words\n",
    "#max_words = len(max((sequences), key=len))\n",
    "#sequences = pad_sequences(sequences,padding='post',maxlen=max_words)\n",
    "max_length=150\n",
    "sequences = pad_sequences(sequences,maxlen=max_length)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1b7604ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,   946,   408,    55,   127,\n",
       "        1565,   233,   493,   125,   370,   277,    59,  4612, 11771,\n",
       "          42,    44,    14,  6809,    47,    80,    63,  3993,    33,\n",
       "        1966,    63,  7261,  5011,   403,    57,    33,   615,   140,\n",
       "         893,   196,  1170,    84,    12,     3,   121, 22909,  2002,\n",
       "         588, 22910, 22911,    32,   152,   518])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "25eabcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,SimpleRNN,Embedding,Flatten,Dropout,GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a24b622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1431, 150)\n",
      "(1431,)\n",
      "(614, 150)\n",
      "(614,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, Y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "96121a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 150, 150)          6335850   \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 150)               180600    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 151       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,516,601\n",
      "Trainable params: 6,516,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embd_len=150\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size+1,output_dim=embd_len,input_length=max_length))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(LSTM(embd_len))\n",
    "#model.add(Dropout(rate = 0.5))\n",
    "#model.add(Dense(units = 32,  activation = 'relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(GlobalMaxPooling1D( ))\n",
    "#model.add(Dense(64,activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2821c1d2-4ed5-4023-85ac-f9afb64c5dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 3s 39ms/step - loss: 0.6424 - accuracy: 0.6303 - val_loss: 0.5593 - val_accuracy: 0.7182\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 30ms/step - loss: 0.3185 - accuracy: 0.8798 - val_loss: 0.6756 - val_accuracy: 0.7085\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 30ms/step - loss: 0.0685 - accuracy: 0.9818 - val_loss: 0.8860 - val_accuracy: 0.7003\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 0.0683 - accuracy: 0.9853 - val_loss: 0.7332 - val_accuracy: 0.7101\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 0.0295 - accuracy: 0.9944 - val_loss: 1.0314 - val_accuracy: 0.7003\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 0.0232 - accuracy: 0.9937 - val_loss: 0.8781 - val_accuracy: 0.7052\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 30ms/step - loss: 0.0372 - accuracy: 0.9895 - val_loss: 0.7331 - val_accuracy: 0.7296\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 32ms/step - loss: 0.0212 - accuracy: 0.9951 - val_loss: 0.9719 - val_accuracy: 0.7068\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 30ms/step - loss: 0.0135 - accuracy: 0.9972 - val_loss: 1.0370 - val_accuracy: 0.7003\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 30ms/step - loss: 0.0148 - accuracy: 0.9958 - val_loss: 0.9023 - val_accuracy: 0.7052\n",
      "\n",
      "RNN Score on test dataset--->  [0.9022538661956787, 0.7052116990089417]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_test,y_test))\n",
    "# Printing model score on test data\n",
    "print()\n",
    "print(\"RNN Score on test dataset---> \", model.evaluate(X_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e5c3226-2d96-413c-9e98-5c80407382a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "News_type = df[['type','language']]\n",
    "\n",
    "X2 = News_type.values\n",
    "\n",
    "y2 = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a534a4e3-d893-49e1-9096-6adbd887827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2045, 2)\n"
     ]
    }
   ],
   "source": [
    "print (X2.shape)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f6f5bd3f-28c9-4838-ae65-09dd0c2a04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# label_encoder object knows how to understand word labels.\n",
    "feature_encoder1 = preprocessing.OneHotEncoder()\n",
    "\n",
    "# Encode labels in column 'species'.\n",
    "XX2 = feature_encoder1.fit_transform(X2)\n",
    "label_encoder2 = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode labels in column 'species'.\n",
    "yy2 = label_encoder2.fit_transform(y2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX2, yy2, test_size=0.20, random_state=42)\n",
    "\n",
    "#from keras.utils import to_categorical\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b3883f0f-d1ca-43bc-86ca-988783eaf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input2 = Input(shape=(13,))\n",
    "dense_layer_1 = Dense(32, activation='relu')(input2)\n",
    "dense_layer_2 = Dense(32, activation='relu')(dense_layer_1)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer_2)\n",
    "\n",
    "model = Model(inputs=input2, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "af9bf93c-b109-4084-b60f-92ee3dd82aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 13)]              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                448       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,537\n",
      "Trainable params: 1,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "473f353c-3dc7-4d18-96c2-b3252f4fd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input2, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b6406015-913a-4e47-a450-7a9567555335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\my_env\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_2/dense_18/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_2/dense_18/embedding_lookup_sparse/Reshape:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_2/dense_18/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 1s 7ms/step - loss: 0.0000e+00 - acc: 0.6167 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 1s 6ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 1s 7ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 1s 8ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=16, epochs=10, verbose=1, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c29e335b-f05a-469a-9381-0b3158dfa433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - acc: 0.6455\n",
      "Test Score: 0.0\n",
      "Test Accuracy: 0.645476758480072\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dd8d05f3-4836-4944-bf2a-f8be271534e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "News_type = df[['type','language']]\n",
    "\n",
    "X2 = News_type.values\n",
    "\n",
    "y2 = df['label']\n",
    "News_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "eab306be-567e-4409-a746-05f17e58aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "#vocab_size=5000\n",
    "tokenizer = Tokenizer( )\n",
    "#tokenizer = Tokenizer(num_words=vocab_size )\n",
    "X2 = df['type'] + ' ' + df['language']\n",
    "\n",
    "tokenizer.fit_on_texts(X2)\n",
    "w_idx=tokenizer.word_index\n",
    "vocab_size=len(w_idx)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b9e9f4ba-e13d-4c9e-b3e6-be96e70e7ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 1,\n",
       " 'bs': 2,\n",
       " 'conspiracy': 3,\n",
       " 'bias': 4,\n",
       " 'hate': 5,\n",
       " 'satire': 6,\n",
       " 'state': 7,\n",
       " 'junksci': 8,\n",
       " 'german': 9,\n",
       " 'fake': 10,\n",
       " 'ignore': 11,\n",
       " 'french': 12,\n",
       " 'spanish': 13}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5672117f-e390-4493-9aef-dce2c0f30de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2045, 2)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences1 = tokenizer.texts_to_sequences(X2)\n",
    "sequences1 = pad_sequences(sequences1)\n",
    "sequences1.shape\n",
    "sequences1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "117765ff-5f22-441f-9581-5c0c6616c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences1, yy2, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9c1b436b-1b7c-4565-a06f-4bd7adcd650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 10)                30        \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.0000e+00 - acc: 0.6271 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - acc: 0.6278 - val_loss: 0.0000e+00 - val_acc: 0.6455\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - acc: 0.6455\n",
      "Test Score: 0.0\n",
      "Test Accuracy: 0.645476758480072\n"
     ]
    }
   ],
   "source": [
    "input2 = Input(shape=(2,))\n",
    "dense_layer_1 = Dense(10, activation='relu')(input2)\n",
    "dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer_2)\n",
    "\n",
    "model = Model(inputs=input2, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "model = Model(inputs=input2, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=16, epochs=10, verbose=1, validation_data=(X_test,y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5712e-cd98-46cd-affe-a3bf2c2b3401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
